{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pavlosemyvolos/Assessment/blob/main/Lab6_7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CRdGNUAfcBFC"
      },
      "outputs": [],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.0.0/spark-3.0.0-bin-hadoop3.2.tgz\n",
        "\n",
        "!tar xf spark-3.0.0-bin-hadoop3.2.tgz\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = '/content/spark-3.0.0-bin-hadoop3.2'\n",
        "\n",
        "!pip install -q findspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DaDCzi-9cSTx",
        "outputId": "e2be58d4-8440-4967-d672-55518518c72e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark==3.0.0 in /usr/local/lib/python3.10/dist-packages (3.0.0)\n",
            "Requirement already satisfied: py4j==0.10.9 in /usr/local/lib/python3.10/dist-packages (from pyspark==3.0.0) (0.10.9)\n"
          ]
        }
      ],
      "source": [
        "!pip3 install pyspark==3.0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "id": "0ucDY7-DcidP",
        "outputId": "e4780743-b03f-4c1a-ce5b-2310f8200a0c"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "Exception",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/findspark.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(spark_home, python_path, edit_rc, edit_profile)\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m             \u001b[0mpy4j\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspark_python\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"lib\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"py4j-*.zip\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-70db33e13406>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfindspark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfindspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/spark-3.0.0-bin-hadoop3.2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/findspark.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(spark_home, python_path, edit_rc, edit_profile)\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0mpy4j\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspark_python\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"lib\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"py4j-*.zip\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m             raise Exception(\n\u001b[0m\u001b[1;32m    162\u001b[0m                 \"Unable to find py4j in {}, your SPARK_HOME may not be configured correctly\".format(\n\u001b[1;32m    163\u001b[0m                     \u001b[0mspark_python\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mException\u001b[0m: Unable to find py4j in /content/spark-3.0.0-bin-hadoop3.2/python, your SPARK_HOME may not be configured correctly"
          ]
        }
      ],
      "source": [
        "import findspark\n",
        "findspark.init(\"/content/spark-3.0.0-bin-hadoop3.2\")\n",
        "from pyspark import SparkConf\n",
        "from pyspark.sql import SparkSession\n",
        "import pyspark.sql.types as t\n",
        "import pandas as pd\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
        "from pyspark.sql.functions import col, count, avg\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.window import Window"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UjERsSJkct0j"
      },
      "outputs": [],
      "source": [
        "spark = (SparkSession.builder.master(\"local\").appName(\"WordCount\").config(\"spark.some.config.option\", \"some-value\").getOrCreate())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pm7Rb5hbczS9"
      },
      "outputs": [],
      "source": [
        "!spark-submit --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Srr4YzIjc4TJ"
      },
      "outputs": [],
      "source": [
        "readAkas = spark.read.csv(\"akas.tsv\", sep=r'\\t',header=True)\n",
        "readAkas.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bsWIzftphGcP"
      },
      "outputs": [],
      "source": [
        "readName_Basics = spark.read.csv(\"name_basics.tsv\", sep=r'\\t',header=True)\n",
        "readName_Basics.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WO46Zs2w0PYw"
      },
      "outputs": [],
      "source": [
        "readTitle_Basics = spark.read.csv(\"title_basics.tsv\", sep=r'\\t',header=True)\n",
        "readTitle_Basics.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oRCXtwQCh1Qt"
      },
      "outputs": [],
      "source": [
        "readEpisode = spark.read.csv(\"episode.tsv\", sep=r'\\t',header=True)\n",
        "readEpisode.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1E672CIrh-Jq"
      },
      "outputs": [],
      "source": [
        "readCrew = spark.read.csv(\"crew.tsv\", sep=r'\\t',header=True)\n",
        "readCrew.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rCF9iwyyiVbL"
      },
      "outputs": [],
      "source": [
        "Ratings = spark.read.csv(\"ratings.tsv\", sep=r'\\t',header=True)\n",
        "Ratings.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YkdPrEe8igps"
      },
      "outputs": [],
      "source": [
        "spark = SparkSession.builder.appName(\"example\").getOrCreate()\n",
        "df = pd.DataFrame({'First_Name': ['Pavlo', 'Sergiy', 'Mtkola'], 'Last_Name': ['Semyvolos','Konstantionov','Zdoryk']})\n",
        "schema = StructType(fields=[\n",
        "    StructField(\"First_Name\", StringType()),\n",
        "    StructField(\"Last_Name\", StringType())\n",
        "])\n",
        "spark_df = spark.createDataFrame(df, schema=schema)\n",
        "spark_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5KH42z8Ui212"
      },
      "outputs": [],
      "source": [
        "spark = SparkSession.builder.appName(\"example\").getOrCreate()\n",
        "\n",
        "schema = StructType(fields=[\n",
        "    StructField(\"nconst\", StringType(), nullable=True),\n",
        "    StructField(\"primaryName\", StringType(), nullable=True),\n",
        "    StructField(\"birthYear\", DoubleType(), nullable=True),\n",
        "    StructField(\"deathYear\", StringType(), nullable=True),\n",
        "    StructField(\"primaryProfession\", StringType(), nullable=True),\n",
        "    StructField(\"knownForTitles\", StringType(), nullable=True)\n",
        "])\n",
        "\n",
        "name_basics_df = spark.read.csv(\"basics.tsv\", sep='\\t', nullValue='\\\\N', header=True, schema=schema)\n",
        "\n",
        "name_basics_df.printSchema()\n",
        "name_basics_df.show(7, truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XbPbLQW-lK9V"
      },
      "outputs": [],
      "source": [
        "Akas = spark.read.csv(\"akas.tsv\", sep=r'\\t',header=True)\n",
        "Akas.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r4kBJAwml4Yx"
      },
      "outputs": [],
      "source": [
        "ukrainian_titles = Akas.filter((col(\"language\") == \"uk\") | (col(\"language\") == \"UA\")).select(\"title\")\n",
        "ukrainian_titles.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A6j1ep9XmZ-p"
      },
      "outputs": [],
      "source": [
        "output_path = \"output1.csv\"\n",
        "ukrainian_titles.write.mode(\"overwrite\").option(\"header\", \"true\").csv(output_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0HorSGICmonH"
      },
      "outputs": [],
      "source": [
        "First = spark.read.csv(\"output1.csv\", sep=r',',header=True)\n",
        "First.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E8wSFQR4nLBc"
      },
      "outputs": [],
      "source": [
        "readTitle_Basics.show(7, truncate=False)\n",
        "Ratings.show(7, truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y_T4ht_31mUa"
      },
      "outputs": [],
      "source": [
        "merged_data = readTitle_Basics.join(Ratings, 'tconst')\n",
        "\n",
        "windowSpec = Window.partitionBy('genres').orderBy(F.desc('numVotes'))\n",
        "\n",
        "ranked_movies = merged_data.withColumn('rank', F.dense_rank().over(windowSpec))\n",
        "\n",
        "top_movies_by_genre = ranked_movies.filter(F.col('rank') <= 10) \\\n",
        "    .select('genres', 'primaryTitle', 'averageRating', 'numVotes')\n",
        "\n",
        "top_movies_by_genre.show(truncate=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNDxCr11UuYdzsVLrWpuxiy",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}